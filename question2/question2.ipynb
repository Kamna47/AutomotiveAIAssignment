{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7369b9-ee38-49d8-8914-a329c1fc4fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MEDIAPIPE DLL ERROR FIX\n",
      "================================================================================\n",
      "\n",
      "üîß Attempting to fix MediaPipe DLL error...\n",
      "\n",
      "This error usually occurs due to:\n",
      "1. Missing Visual C++ Redistributables\n",
      "2. Incompatible MediaPipe version\n",
      "3. Conflicting package versions\n",
      "\n",
      "================================================================================\n",
      "SOLUTION: Reinstalling packages with correct versions\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Running installation commands...\n",
      "\n",
      "Step 1/7: uninstall mediapipe -y\n",
      "   ‚úì Success\n",
      "\n",
      "Step 2/7: uninstall opencv-python -y\n",
      "   ‚úì Success\n",
      "\n",
      "Step 3/7: uninstall opencv-contrib-python -y\n",
      "   ‚ö† Warning: 2\n",
      "\n",
      "Step 4/7: install opencv-python==4.8.1.78\n",
      "   ‚úì Success\n",
      "\n",
      "Step 5/7: install mediapipe==0.10.9\n",
      "   ‚úì Success\n",
      "\n",
      "Step 6/7: install numpy==1.24.3\n",
      "   ‚úì Success\n",
      "\n",
      "Step 7/7: install pandas>=2.0.0\n",
      "   ‚úì Success\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ INSTALLATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üîÑ Please RESTART your Jupyter kernel and try again!\n",
      "\n",
      "In Jupyter: Kernel ‚Üí Restart Kernel\n",
      "\n",
      "================================================================================\n",
      "TESTING IMPORT\n",
      "================================================================================\n",
      "‚úì OpenCV imported successfully (version 4.11.0)\n",
      "‚úì MediaPipe imported successfully (version 0.10.9)\n",
      "‚úì NumPy imported successfully (version 1.26.4)\n",
      "‚úì Pandas imported successfully (version 2.1.2)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MEDIAPIPE DLL FIX AND SETUP SCRIPT\n",
    "Run this first to fix the MediaPipe import error\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MEDIAPIPE DLL ERROR FIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîß Attempting to fix MediaPipe DLL error...\")\n",
    "print(\"\\nThis error usually occurs due to:\")\n",
    "print(\"1. Missing Visual C++ Redistributables\")\n",
    "print(\"2. Incompatible MediaPipe version\")\n",
    "print(\"3. Conflicting package versions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SOLUTION: Reinstalling packages with correct versions\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "commands = [\n",
    "    # Uninstall existing mediapipe\n",
    "    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"mediapipe\", \"-y\"],\n",
    "    \n",
    "    # Uninstall opencv if conflicting\n",
    "    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"opencv-python\", \"-y\"],\n",
    "    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"opencv-contrib-python\", \"-y\"],\n",
    "    \n",
    "    # Install specific working versions\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python==4.8.1.78\"],\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"mediapipe==0.10.9\"],\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.24.3\"],\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"pandas>=2.0.0\"],\n",
    "]\n",
    "\n",
    "print(\"\\n‚è≥ Running installation commands...\\n\")\n",
    "\n",
    "for i, cmd in enumerate(commands, 1):\n",
    "    print(f\"Step {i}/{len(commands)}: {' '.join(cmd[3:])}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úì Success\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† Warning: {result.returncode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Error: {e}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ INSTALLATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîÑ Please RESTART your Jupyter kernel and try again!\")\n",
    "print(\"\\nIn Jupyter: Kernel ‚Üí Restart Kernel\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING IMPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"‚úì OpenCV imported successfully (version {cv2.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó OpenCV import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    print(f\"‚úì MediaPipe imported successfully (version {mp.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó MediaPipe import failed: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è If MediaPipe still fails after restart, try these additional steps:\")\n",
    "    print(\"\\n1. Install Visual C++ Redistributable:\")\n",
    "    print(\"   Download from: https://aka.ms/vs/17/release/vc_redist.x64.exe\")\n",
    "    print(\"\\n2. Or use alternative method (Conda):\")\n",
    "    print(\"   conda install -c conda-forge mediapipe\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úì NumPy imported successfully (version {np.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó NumPy import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"‚úì Pandas imported successfully (version {pd.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Pandas import failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d239fd-6275-4def-a43f-b6dd27bae5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DRIVER ACTIVITY MONITORING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "üì¶ Loading libraries...\n",
      "‚úì Output directory: C:\\Users\\akhilesh zende\\Downloads\\DriverActivityAnalysis_20260209_084539\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "üìπ Video input: C:\\Users\\akhilesh zende\\Downloads\\WIN_20260208_23_39_16_Pro.mp4\n",
      "üìÅ Output folder: C:\\Users\\akhilesh zende\\Downloads\\DriverActivityAnalysis_20260209_084539\n",
      "üìÑ CSV output: C:\\Users\\akhilesh zende\\Downloads\\DriverActivityAnalysis_20260209_084539\\driver_activity_analysis.csv\n",
      "üé• Video output: C:\\Users\\akhilesh zende\\Downloads\\DriverActivityAnalysis_20260209_084539\\annotated_video.mp4\n",
      "üìù Summary output: C:\\Users\\akhilesh zende\\Downloads\\DriverActivityAnalysis_20260209_084539\\analysis_summary.txt\n",
      "\n",
      "================================================================================\n",
      "STARTING VIDEO ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìπ Input: C:\\Users\\akhilesh zende\\Downloads\\WIN_20260208_23_39_16_Pro.mp4\n",
      "üìä Resolution: 1280x720\n",
      "üìä FPS: 19.91\n",
      "üìä Total frames: 26057\n",
      "üìä Duration: 1308.90 seconds\n",
      "\n",
      "üîß Initializing MediaPipe FaceMesh...\n",
      "\n",
      "üöÄ Processing frames...\n",
      "   ‚è≥ 0% complete (1/26057 frames)\n",
      "   ‚è≥ 5% complete (1303/26057 frames)\n",
      "   ‚è≥ 10% complete (2606/26057 frames)\n",
      "   ‚è≥ 15% complete (3909/26057 frames)\n",
      "   ‚è≥ 20% complete (5212/26057 frames)\n",
      "   ‚è≥ 25% complete (6515/26057 frames)\n",
      "   ‚è≥ 30% complete (7818/26057 frames)\n",
      "   ‚è≥ 35% complete (9120/26057 frames)\n",
      "   ‚è≥ 40% complete (10423/26057 frames)\n",
      "   ‚è≥ 45% complete (11726/26057 frames)\n",
      "   ‚è≥ 50% complete (13029/26057 frames)\n",
      "   ‚è≥ 55% complete (14332/26057 frames)\n",
      "   ‚è≥ 60% complete (15635/26057 frames)\n",
      "   ‚è≥ 65% complete (16938/26057 frames)\n",
      "   ‚è≥ 70% complete (18240/26057 frames)\n",
      "   ‚è≥ 75% complete (19543/26057 frames)\n",
      "   ‚è≥ 80% complete (20846/26057 frames)\n",
      "   ‚è≥ 85% complete (22149/26057 frames)\n",
      "   ‚è≥ 90% complete (23452/26057 frames)\n",
      "   ‚è≥ 95% complete (24755/26057 frames)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DRIVER ACTIVITY MONITORING SYSTEM - JUPYTER NOTEBOOK VERSION\n",
    "Complete standalone script for analyzing driver face videos\n",
    "\n",
    "Run this entire cell in Jupyter Notebook to:\n",
    "1. Process your face video\n",
    "2. Generate CSV with frame-by-frame analysis\n",
    "3. Create annotated video with visualizations\n",
    "4. Save outputs to Downloads folder\n",
    "\n",
    "Author: Enhanced from reference code\n",
    "Date: February 2026\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Video\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DRIVER ACTIVITY MONITORING SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüì¶ Loading libraries...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - CHANGE THESE VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# YOUR VIDEO FILE PATH - UPDATE THIS!\n",
    "VIDEO_PATH = r\"C:\\Users\\akhilesh zende\\Downloads\\WIN_20260208_23_39_16_Pro.mp4\"\n",
    "\n",
    "# OUTPUT FOLDER - Will be created in Downloads\n",
    "OUTPUT_FOLDER_NAME = \"DriverActivityAnalysis_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create output directory in Downloads folder\n",
    "DOWNLOADS_PATH = str(Path.home() / \"Downloads\")\n",
    "OUTPUT_DIR = os.path.join(DOWNLOADS_PATH, OUTPUT_FOLDER_NAME)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Output file paths\n",
    "CSV_OUTPUT = os.path.join(OUTPUT_DIR, \"driver_activity_analysis.csv\")\n",
    "VIDEO_OUTPUT = os.path.join(OUTPUT_DIR, \"annotated_video.mp4\")\n",
    "SUMMARY_OUTPUT = os.path.join(OUTPUT_DIR, \"analysis_summary.txt\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def clamp(x, minimum, maximum):\n",
    "    \"\"\"Clamp value between min and max\"\"\"\n",
    "    return max(minimum, min(maximum, x))\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "def safe_divide(num, denom, default=0.0):\n",
    "    \"\"\"Safe division avoiding divide by zero\"\"\"\n",
    "    return num / denom if abs(denom) > 1e-6 else default\n",
    "\n",
    "# ============================================================================\n",
    "# EYE ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_eye_aspect_ratio(eye_points):\n",
    "    \"\"\"\n",
    "    Calculate Eye Aspect Ratio (EAR) for blink detection\n",
    "    EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)\n",
    "    \"\"\"\n",
    "    if len(eye_points) != 6:\n",
    "        return 0.25\n",
    "    \n",
    "    p1, p2, p3, p4, p5, p6 = eye_points\n",
    "    \n",
    "    vertical_1 = euclidean_distance(p2, p6)\n",
    "    vertical_2 = euclidean_distance(p3, p5)\n",
    "    horizontal = euclidean_distance(p1, p4)\n",
    "    \n",
    "    ear = safe_divide(vertical_1 + vertical_2, 2.0 * horizontal, 0.25)\n",
    "    return ear\n",
    "\n",
    "def classify_eye_state(ear, threshold=0.23):\n",
    "    \"\"\"Classify if eyes are open or closed\"\"\"\n",
    "    return \"eyeClosed\" if ear < threshold else \"eyeOpen\"\n",
    "\n",
    "# ============================================================================\n",
    "# GAZE/PUPIL ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_iris_ratio(iris_points, left_corner, right_corner):\n",
    "    \"\"\"Calculate horizontal position of iris in eye (0=left, 0.5=center, 1=right)\"\"\"\n",
    "    if len(iris_points) == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    iris_center = np.mean(iris_points, axis=0)\n",
    "    eye_width = euclidean_distance(left_corner, right_corner)\n",
    "    \n",
    "    if eye_width < 1e-6:\n",
    "        return 0.5\n",
    "    \n",
    "    iris_offset = iris_center[0] - left_corner[0]\n",
    "    ratio = clamp(iris_offset / eye_width, 0.0, 1.0)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "def classify_pupil_position(ratio):\n",
    "    \"\"\"Classify gaze direction based on iris position\"\"\"\n",
    "    if ratio < 0.38:\n",
    "        return \"left\"\n",
    "    elif ratio > 0.62:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"center\"\n",
    "\n",
    "# ============================================================================\n",
    "# HEAD POSE ESTIMATION\n",
    "# ============================================================================\n",
    "\n",
    "def estimate_head_pose(landmarks, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Estimate head pose angles (yaw, pitch, roll) using PnP algorithm\n",
    "    Returns: (yaw, pitch, roll) in degrees\n",
    "    \"\"\"\n",
    "    \n",
    "    # Key facial points for pose estimation\n",
    "    indices = {\n",
    "        \"nose_tip\": 1,\n",
    "        \"chin\": 152,\n",
    "        \"left_eye\": 33,\n",
    "        \"right_eye\": 263,\n",
    "        \"left_mouth\": 61,\n",
    "        \"right_mouth\": 291\n",
    "    }\n",
    "    \n",
    "    # 2D image points\n",
    "    image_pts = np.array([\n",
    "        [landmarks[indices[\"nose_tip\"]].x * img_width, \n",
    "         landmarks[indices[\"nose_tip\"]].y * img_height],\n",
    "        [landmarks[indices[\"chin\"]].x * img_width, \n",
    "         landmarks[indices[\"chin\"]].y * img_height],\n",
    "        [landmarks[indices[\"left_eye\"]].x * img_width, \n",
    "         landmarks[indices[\"left_eye\"]].y * img_height],\n",
    "        [landmarks[indices[\"right_eye\"]].x * img_width, \n",
    "         landmarks[indices[\"right_eye\"]].y * img_height],\n",
    "        [landmarks[indices[\"left_mouth\"]].x * img_width, \n",
    "         landmarks[indices[\"left_mouth\"]].y * img_height],\n",
    "        [landmarks[indices[\"right_mouth\"]].x * img_width, \n",
    "         landmarks[indices[\"right_mouth\"]].y * img_height]\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    # 3D model points (generic face model)\n",
    "    model_pts = np.array([\n",
    "        (0.0, 0.0, 0.0),          # Nose tip\n",
    "        (0.0, -63.6, -12.5),      # Chin\n",
    "        (-43.3, 32.7, -26.0),     # Left eye\n",
    "        (43.3, 32.7, -26.0),      # Right eye\n",
    "        (-28.9, -28.9, -24.1),    # Left mouth\n",
    "        (28.9, -28.9, -24.1)      # Right mouth\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    # Camera matrix\n",
    "    focal_length = img_width\n",
    "    center = (img_width / 2, img_height / 2)\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    dist_coeffs = np.zeros((4, 1))\n",
    "    \n",
    "    # Solve PnP\n",
    "    success, rot_vec, trans_vec = cv2.solvePnP(\n",
    "        model_pts, image_pts, camera_matrix, dist_coeffs,\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Convert to rotation matrix\n",
    "    rot_mat, _ = cv2.Rodrigues(rot_vec)\n",
    "    \n",
    "    # Extract Euler angles\n",
    "    sy = math.sqrt(rot_mat[0, 0]**2 + rot_mat[1, 0]**2)\n",
    "    singular = sy < 1e-6\n",
    "    \n",
    "    if not singular:\n",
    "        x = math.atan2(rot_mat[2, 1], rot_mat[2, 2])\n",
    "        y = math.atan2(-rot_mat[2, 0], sy)\n",
    "        z = math.atan2(rot_mat[1, 0], rot_mat[0, 0])\n",
    "    else:\n",
    "        x = math.atan2(-rot_mat[1, 2], rot_mat[1, 1])\n",
    "        y = math.atan2(-rot_mat[2, 0], sy)\n",
    "        z = 0\n",
    "    \n",
    "    pitch = np.degrees(x)\n",
    "    yaw = np.degrees(y)\n",
    "    roll = np.degrees(z)\n",
    "    \n",
    "    return yaw, pitch, roll\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSIFICATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def classify_head_rotation(yaw):\n",
    "    \"\"\"Classify horizontal head rotation\"\"\"\n",
    "    if yaw > 35:\n",
    "        return \"overLeftShoulder\"\n",
    "    elif yaw > 15:\n",
    "        return \"45DegreeLeft\"\n",
    "    elif yaw < -35:\n",
    "        return \"overRightShoulder\"\n",
    "    elif yaw < -15:\n",
    "        return \"45DegreeRight\"\n",
    "    else:\n",
    "        return \"center\"\n",
    "\n",
    "def classify_head_tilt_front(pitch):\n",
    "    \"\"\"Classify forward/backward head tilt\"\"\"\n",
    "    if pitch < -20:\n",
    "        return \"extremeBackTilt\"\n",
    "    elif pitch < -10:\n",
    "        return \"backTilt\"\n",
    "    elif pitch > 20:\n",
    "        return \"extremeFrontTilt\"\n",
    "    elif pitch > 10:\n",
    "        return \"frontTilt\"\n",
    "    else:\n",
    "        return \"straightFrontAhead\"\n",
    "\n",
    "def classify_head_tilt_side(roll):\n",
    "    \"\"\"Classify left/right head tilt\"\"\"\n",
    "    if roll > 15:\n",
    "        return \"headTopRight_NeckLeft\"\n",
    "    elif roll < -15:\n",
    "        return \"headTopLeft_NeckRight\"\n",
    "    else:\n",
    "        return \"straight\"\n",
    "\n",
    "def classify_driving_status(yaw, pitch, blink_rate, gaze_ratio, avg_ear):\n",
    "    \"\"\"Classify overall driver attention status\"\"\"\n",
    "    \n",
    "    # Distracted: looking away significantly\n",
    "    if abs(yaw) > 25 or abs(pitch) > 25:\n",
    "        return \"distracted\"\n",
    "    \n",
    "    # Drowsy: eyes mostly closed or very high blink\n",
    "    if avg_ear < 0.20 or blink_rate > 35:\n",
    "        return \"drowsy\"\n",
    "    \n",
    "    # Extreme focus: very stable and centered\n",
    "    if abs(yaw) < 8 and abs(pitch) < 8 and gaze_ratio > 0.75 and blink_rate < 12:\n",
    "        return \"extremeFocus\"\n",
    "    \n",
    "    # Focus: good attention\n",
    "    if gaze_ratio > 0.60 and abs(yaw) < 15:\n",
    "        return \"Focus\"\n",
    "    \n",
    "    # Relaxed/bored: high blink but not distracted\n",
    "    if blink_rate > 25:\n",
    "        return \"Relaxed/bore\"\n",
    "    \n",
    "    return \"Normal\"\n",
    "\n",
    "# ============================================================================\n",
    "# BLINK DETECTOR CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class BlinkDetector:\n",
    "    \"\"\"Tracks blink events and calculates blink rate\"\"\"\n",
    "    \n",
    "    def __init__(self, window_sec=60.0):\n",
    "        self.blinks = []\n",
    "        self.was_closed = False\n",
    "        self.window = window_sec\n",
    "    \n",
    "    def update(self, eye_state, timestamp):\n",
    "        \"\"\"Update with current eye state\"\"\"\n",
    "        if eye_state == \"eyeClosed\" and not self.was_closed:\n",
    "            self.was_closed = True\n",
    "        elif eye_state == \"eyeOpen\" and self.was_closed:\n",
    "            self.was_closed = False\n",
    "            self.blinks.append(timestamp)\n",
    "        \n",
    "        # Remove old events\n",
    "        self.blinks = [t for t in self.blinks if timestamp - t <= self.window]\n",
    "    \n",
    "    def get_rate(self):\n",
    "        \"\"\"Get blinks per minute\"\"\"\n",
    "        return float(len(self.blinks))\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def process_driver_video(video_path, csv_path, video_out_path):\n",
    "    \"\"\"\n",
    "    Main processing function\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STARTING VIDEO ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check video exists\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"\\n‚ùå ERROR: Video not found at: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìπ Input: {video_path}\")\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå ERROR: Cannot open video file\")\n",
    "        return None\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps <= 1:\n",
    "        fps = 30.0\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"üìä Resolution: {width}x{height}\")\n",
    "    print(f\"üìä FPS: {fps:.2f}\")\n",
    "    print(f\"üìä Total frames: {total_frames}\")\n",
    "    print(f\"üìä Duration: {total_frames/fps:.2f} seconds\")\n",
    "    \n",
    "    # Setup video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(video_out_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Initialize MediaPipe\n",
    "    print(\"\\nüîß Initializing MediaPipe FaceMesh...\")\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    # Landmark indices\n",
    "    LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "    RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "    LEFT_IRIS = [468, 469, 470, 471, 472]\n",
    "    RIGHT_IRIS = [473, 474, 475, 476, 477]\n",
    "    \n",
    "    # Initialize tracking\n",
    "    blink_detector = BlinkDetector()\n",
    "    gaze_center_count = 0\n",
    "    frames_with_face = 0\n",
    "    results_list = []\n",
    "    \n",
    "    print(\"\\nüöÄ Processing frames...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    frame_num = 0\n",
    "    last_percent = -1\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_num += 1\n",
    "        timestamp = frame_num / fps\n",
    "        \n",
    "        # Progress indicator\n",
    "        percent = int((frame_num / total_frames) * 100)\n",
    "        if percent != last_percent and percent % 5 == 0:\n",
    "            print(f\"   ‚è≥ {percent}% complete ({frame_num}/{total_frames} frames)\")\n",
    "            last_percent = percent\n",
    "        \n",
    "        # Process with MediaPipe\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = face_mesh.process(rgb)\n",
    "        \n",
    "        # Default values\n",
    "        eye_state = \"noFace\"\n",
    "        pupil_pos = \"noFace\"\n",
    "        drive_status = \"noFace\"\n",
    "        head_rot = \"noFace\"\n",
    "        head_front = \"noFace\"\n",
    "        head_side = \"noFace\"\n",
    "        yaw = pitch = roll = 0.0\n",
    "        ear = 0.0\n",
    "        blink_rate = 0.0\n",
    "        \n",
    "        if result.multi_face_landmarks:\n",
    "            lm = result.multi_face_landmarks[0].landmark\n",
    "            frames_with_face += 1\n",
    "            \n",
    "            # === EYE STATE ===\n",
    "            left_eye_pts = [(lm[i].x * width, lm[i].y * height) for i in LEFT_EYE]\n",
    "            right_eye_pts = [(lm[i].x * width, lm[i].y * height) for i in RIGHT_EYE]\n",
    "            \n",
    "            ear_l = calculate_eye_aspect_ratio(left_eye_pts)\n",
    "            ear_r = calculate_eye_aspect_ratio(right_eye_pts)\n",
    "            ear = (ear_l + ear_r) / 2.0\n",
    "            \n",
    "            eye_state = classify_eye_state(ear)\n",
    "            blink_detector.update(eye_state, timestamp)\n",
    "            blink_rate = blink_detector.get_rate()\n",
    "            \n",
    "            # === PUPIL POSITION ===\n",
    "            left_iris_pts = np.array([(lm[i].x * width, lm[i].y * height) for i in LEFT_IRIS])\n",
    "            left_corner = np.array([lm[33].x * width, lm[33].y * height])\n",
    "            right_corner = np.array([lm[133].x * width, lm[133].y * height])\n",
    "            \n",
    "            iris_ratio = calculate_iris_ratio(left_iris_pts, left_corner, right_corner)\n",
    "            pupil_pos = classify_pupil_position(iris_ratio)\n",
    "            \n",
    "            if pupil_pos == \"center\":\n",
    "                gaze_center_count += 1\n",
    "            \n",
    "            gaze_ratio = gaze_center_count / max(frames_with_face, 1)\n",
    "            \n",
    "            # === HEAD POSE ===\n",
    "            yaw, pitch, roll = estimate_head_pose(lm, width, height)\n",
    "            head_rot = classify_head_rotation(yaw)\n",
    "            head_front = classify_head_tilt_front(pitch)\n",
    "            head_side = classify_head_tilt_side(roll)\n",
    "            \n",
    "            # === DRIVING STATUS ===\n",
    "            drive_status = classify_driving_status(yaw, pitch, blink_rate, gaze_ratio, ear)\n",
    "            \n",
    "            # === DRAW ANNOTATIONS ===\n",
    "            y_pos = 30\n",
    "            line_h = 30\n",
    "            \n",
    "            # Eye state\n",
    "            color = (0, 255, 0) if eye_state == \"eyeOpen\" else (0, 165, 255)\n",
    "            cv2.putText(frame, f\"a1 Eye: {eye_state}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, color, 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Pupil\n",
    "            cv2.putText(frame, f\"a2 Pupil: {pupil_pos}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Status\n",
    "            status_colors = {\n",
    "                \"extremeFocus\": (0, 255, 0),\n",
    "                \"Focus\": (0, 200, 0),\n",
    "                \"Normal\": (0, 255, 255),\n",
    "                \"Relaxed/bore\": (0, 165, 255),\n",
    "                \"distracted\": (0, 100, 255),\n",
    "                \"drowsy\": (0, 0, 255)\n",
    "            }\n",
    "            color = status_colors.get(drive_status, (255, 255, 255))\n",
    "            cv2.putText(frame, f\"a3 Status: {drive_status}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, color, 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Head rotation\n",
    "            cv2.putText(frame, f\"a4 Rotation: {head_rot}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 180, 0), 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Front tilt\n",
    "            cv2.putText(frame, f\"a5 Front: {head_front}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 180, 0), 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Side tilt\n",
    "            cv2.putText(frame, f\"a6 Side: {head_side}\", (15, y_pos),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 180, 0), 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Angles\n",
    "            cv2.putText(frame, f\"Yaw:{yaw:.1f} Pitch:{pitch:.1f} Roll:{roll:.1f}\", \n",
    "                       (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 200, 255), 2)\n",
    "            y_pos += line_h\n",
    "            \n",
    "            # Stats\n",
    "            cv2.putText(frame, f\"Blink:{blink_rate:.1f}/min EAR:{ear:.3f}\", \n",
    "                       (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)\n",
    "        \n",
    "        # Save results\n",
    "        results_list.append({\n",
    "            \"frame\": frame_num,\n",
    "            \"time_sec\": round(timestamp, 3),\n",
    "            \"a1_eye_state\": eye_state,\n",
    "            \"a2_pupil_position\": pupil_pos,\n",
    "            \"a3_driving_status\": drive_status,\n",
    "            \"a4_head_rotation\": head_rot,\n",
    "            \"a5_head_front_tilt\": head_front,\n",
    "            \"a6_head_side_tilt\": head_side,\n",
    "            \"yaw_deg\": round(yaw, 2),\n",
    "            \"pitch_deg\": round(pitch, 2),\n",
    "            \"roll_deg\": round(roll, 2),\n",
    "            \"ear\": round(ear, 3),\n",
    "            \"blink_rate_per_min\": round(blink_rate, 1)\n",
    "        })\n",
    "        \n",
    "        # Write frame\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    face_mesh.close()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processing complete!\")\n",
    "    print(f\"   ‚è±Ô∏è  Time: {elapsed:.2f} seconds\")\n",
    "    print(f\"   üìä Frames processed: {frame_num}\")\n",
    "    print(f\"   üìä Frames with face: {frames_with_face}\")\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results_list)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nüíæ CSV saved: {csv_path}\")\n",
    "    print(f\"üíæ Video saved: {video_out_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS AND SUMMARY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_summary(df, output_path):\n",
    "    \"\"\"Generate text summary of analysis\"\"\"\n",
    "    \n",
    "    summary = []\n",
    "    summary.append(\"=\" * 80)\n",
    "    summary.append(\"DRIVER ACTIVITY ANALYSIS SUMMARY\")\n",
    "    summary.append(\"=\" * 80)\n",
    "    summary.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    summary.append(f\"Total frames: {len(df)}\")\n",
    "    \n",
    "    # Filter only frames with face detected\n",
    "    df_face = df[df['a1_eye_state'] != 'noFace']\n",
    "    summary.append(f\"Frames with face: {len(df_face)} ({len(df_face)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(df_face) > 0:\n",
    "        summary.append(\"\\n\" + \"-\" * 80)\n",
    "        summary.append(\"METRIC DISTRIBUTIONS\")\n",
    "        summary.append(\"-\" * 80)\n",
    "        \n",
    "        # a1: Eye State\n",
    "        summary.append(\"\\na1. EYE STATE:\")\n",
    "        eye_counts = df_face['a1_eye_state'].value_counts()\n",
    "        for state, count in eye_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {state}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # a2: Pupil Position\n",
    "        summary.append(\"\\na2. PUPIL POSITION:\")\n",
    "        pupil_counts = df_face['a2_pupil_position'].value_counts()\n",
    "        for pos, count in pupil_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {pos}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # a3: Driving Status\n",
    "        summary.append(\"\\na3. DRIVING STATUS:\")\n",
    "        status_counts = df_face['a3_driving_status'].value_counts()\n",
    "        for status, count in status_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {status}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # a4: Head Rotation\n",
    "        summary.append(\"\\na4. HEAD ROTATION:\")\n",
    "        rot_counts = df_face['a4_head_rotation'].value_counts()\n",
    "        for rot, count in rot_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {rot}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # a5: Head Front Tilt\n",
    "        summary.append(\"\\na5. HEAD FRONT TILT:\")\n",
    "        front_counts = df_face['a5_head_front_tilt'].value_counts()\n",
    "        for tilt, count in front_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {tilt}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # a6: Head Side Tilt\n",
    "        summary.append(\"\\na6. HEAD SIDE TILT:\")\n",
    "        side_counts = df_face['a6_head_side_tilt'].value_counts()\n",
    "        for tilt, count in side_counts.items():\n",
    "            pct = count / len(df_face) * 100\n",
    "            summary.append(f\"   {tilt}: {count} frames ({pct:.1f}%)\")\n",
    "        \n",
    "        # Statistics\n",
    "        summary.append(\"\\n\" + \"-\" * 80)\n",
    "        summary.append(\"STATISTICS\")\n",
    "        summary.append(\"-\" * 80)\n",
    "        \n",
    "        summary.append(f\"\\nAverage Eye Aspect Ratio: {df_face['ear'].mean():.3f}\")\n",
    "        summary.append(f\"Average Blink Rate: {df_face['blink_rate_per_min'].mean():.1f} per minute\")\n",
    "        summary.append(f\"\\nHead Pose Angles (average):\")\n",
    "        summary.append(f\"   Yaw: {df_face['yaw_deg'].mean():.2f}¬∞\")\n",
    "        summary.append(f\"   Pitch: {df_face['pitch_deg'].mean():.2f}¬∞\")\n",
    "        summary.append(f\"   Roll: {df_face['roll_deg'].mean():.2f}¬∞\")\n",
    "    \n",
    "    summary.append(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    summary_text = \"\\n\".join(summary)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "def display_sample_data(df, n=10):\n",
    "    \"\"\"Display sample rows from dataframe\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SAMPLE DATA (first {n} rows with face detected)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_face = df[df['a1_eye_state'] != 'noFace'].head(n)\n",
    "    \n",
    "    if len(df_face) > 0:\n",
    "        display(df_face)\n",
    "    else:\n",
    "        print(\"No frames with face detected in first rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìπ Video input: {VIDEO_PATH}\")\n",
    "    print(f\"üìÅ Output folder: {OUTPUT_DIR}\")\n",
    "    print(f\"üìÑ CSV output: {CSV_OUTPUT}\")\n",
    "    print(f\"üé• Video output: {VIDEO_OUTPUT}\")\n",
    "    print(f\"üìù Summary output: {SUMMARY_OUTPUT}\")\n",
    "    \n",
    "    # Process video\n",
    "    df = process_driver_video(VIDEO_PATH, CSV_OUTPUT, VIDEO_OUTPUT)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\n‚ùå Processing failed!\")\n",
    "        return\n",
    "    \n",
    "    # Generate summary\n",
    "    print(\"\\nüìù Generating summary...\")\n",
    "    summary = generate_summary(df, SUMMARY_OUTPUT)\n",
    "    print(summary)\n",
    "    \n",
    "    # Display sample data\n",
    "    display_sample_data(df, n=10)\n",
    "    \n",
    "    # Final message\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ ALL DONE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìÅ All outputs saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"\\n   1. {os.path.basename(CSV_OUTPUT)} - Frame-by-frame data\")\n",
    "    print(f\"   2. {os.path.basename(VIDEO_OUTPUT)} - Annotated video\")\n",
    "    print(f\"   3. {os.path.basename(SUMMARY_OUTPUT)} - Analysis summary\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Try to display video in notebook\n",
    "    try:\n",
    "        print(\"\\nüé• Video preview:\")\n",
    "        display(Video(VIDEO_OUTPUT, width=640))\n",
    "    except:\n",
    "        print(\"\\n(Video display not available in this environment)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete analysis\n",
    "    results_df = main()\n",
    "\n",
    "# If running in Jupyter, just execute:\n",
    "# results_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697cd79-6efb-4890-94aa-fb741da13dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
